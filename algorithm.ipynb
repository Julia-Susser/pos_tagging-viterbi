{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c25036-d929-4314-bb8a-1ebc22693c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1387/1387 [00:01<00:00, 761.98it/s]\n",
      "100%|██████████| 462/462 [00:00<00:00, 725.48it/s]\n",
      "100%|██████████| 463/463 [00:00<00:00, 1473.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from tagger_utils import *\n",
    "train_data = load_data(\"data/train_x.csv\", \"data/train_y.csv\")\n",
    "dev_data = load_data(\"data/dev_x.csv\", \"data/dev_y.csv\")\n",
    "test_data = load_data(\"data/test_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0969b993-89b0-4b76-9b78-d15761d8a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004614798929380484\n",
      "0.0004614798929380484\n",
      "['PRP', 'VBP', 'JJ', 'CC', 'PRP', 'VBP', 'PRP']\n",
      "['PRP', 'VBP', 'JJ', 'CC', 'PRP', 'VBP', 'PRP']\n",
      "6 PRP\n",
      "['PRP', 'VBP', 'JJ', 'CC', 'PRP', 'VBP', 'PRP']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Contains the part of speech tagger class. \"\"\"\n",
    "\n",
    "\n",
    "def evaluate(data, model):\n",
    "    \"\"\"Evaluates the POS model on some sentences and gold tags.\n",
    "\n",
    "    This model can compute a few different accuracies:\n",
    "        - whole-sentence accuracy\n",
    "        - per-token accuracy\n",
    "        - compare the probabilities computed by different styles of decoding\n",
    "\n",
    "    You might want to refactor this into several different evaluation functions,\n",
    "    or you can use it as is. \n",
    "    \n",
    "    As per the write-up, you may find it faster to use multiprocessing (code included). \n",
    "    \n",
    "    \"\"\"\n",
    "    processes = 4\n",
    "    sentences = data[0]\n",
    "    tags = data[1]\n",
    "    n = len(sentences)\n",
    "    k = n//processes\n",
    "    n_tokens = sum([len(d) for d in sentences])\n",
    "    unk_n_tokens = sum([1 for s in sentences for w in s if w not in model.word2idx.keys()])\n",
    "    predictions = {i:None for i in range(n)}\n",
    "    probabilities = {i:None for i in range(n)}\n",
    "         \n",
    "    start = time.time()\n",
    "    pool = Pool(processes=processes)\n",
    "    res = []\n",
    "    for i in range(0, n, k):\n",
    "        res.append(pool.apply_async(infer_sentences, [model, sentences[i:i+k], i]))\n",
    "    ans = [r.get(timeout=None) for r in res]\n",
    "    predictions = dict()\n",
    "    for a in ans:\n",
    "        predictions.update(a)\n",
    "    print(f\"Inference Runtime: {(time.time()-start)/60} minutes.\")\n",
    "    \n",
    "    start = time.time()\n",
    "    pool = Pool(processes=processes)\n",
    "    res = []\n",
    "    for i in range(0, n, k):\n",
    "        res.append(pool.apply_async(compute_prob, [model, sentences[i:i+k], tags[i:i+k], i]))\n",
    "    ans = [r.get(timeout=None) for r in res]\n",
    "    probabilities = dict()\n",
    "    for a in ans:\n",
    "        probabilities.update(a)\n",
    "    print(f\"Probability Estimation Runtime: {(time.time()-start)/60} minutes.\")\n",
    "\n",
    "\n",
    "    token_acc = sum([1 for i in range(n) for j in range(len(sentences[i])) if tags[i][j] == predictions[i][j]]) / n_tokens\n",
    "    unk_token_acc = sum([1 for i in range(n) for j in range(len(sentences[i])) if tags[i][j] == predictions[i][j] and sentences[i][j] not in model.word2idx.keys()]) / unk_n_tokens\n",
    "    whole_sent_acc = 0\n",
    "    num_whole_sent = 0\n",
    "    for k in range(n):\n",
    "        sent = sentences[k]\n",
    "        eos_idxes = indices(sent, '.')\n",
    "        start_idx = 1\n",
    "        end_idx = eos_idxes[0]\n",
    "        for i in range(1, len(eos_idxes)):\n",
    "            whole_sent_acc += 1 if tags[k][start_idx:end_idx] == predictions[k][start_idx:end_idx] else 0\n",
    "            num_whole_sent += 1\n",
    "            start_idx = end_idx+1\n",
    "            end_idx = eos_idxes[i]\n",
    "    print(\"Whole sent acc: {}\".format(whole_sent_acc/num_whole_sent))\n",
    "    print(\"Mean Probabilities: {}\".format(sum(probabilities.values())/n))\n",
    "    print(\"Token acc: {}\".format(token_acc))\n",
    "    print(\"Unk token acc: {}\".format(unk_token_acc))\n",
    "    \n",
    "    confusion_matrix(pos_tagger.tag2idx, pos_tagger.idx2tag, predictions.values(), tags, 'cm.png')\n",
    "\n",
    "    return whole_sent_acc/num_whole_sent, token_acc, sum(probabilities.values())/n\n",
    "\n",
    "\n",
    "class POSTagger():\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the tagger model parameters and anything else necessary. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_unigrams(self):\n",
    "        \"\"\"\n",
    "        Computes unigrams. \n",
    "        Tip. Map each tag to an integer and store the unigrams in a numpy array. \n",
    "        \"\"\"\n",
    "        n_tags = len(self.tag2idx.keys())\n",
    "        self.unigram = np.zeros((n_tags))\n",
    "        for tag in train_data[1]:\n",
    "            for t in tag:\n",
    "                tag_idx = self.tag2idx[t]\n",
    "                self.unigram[tag_idx]= self.unigram[tag_idx]+1\n",
    "        \n",
    "\n",
    "\n",
    "    def get_bigrams(self):        \n",
    "        \"\"\"\n",
    "        Computes bigrams. \n",
    "        Tip. Map each tag to an integer and store the bigrams in a numpy array\n",
    "             such that bigrams[index[tag1], index[tag2]] = Prob(tag2|tag1). \n",
    "        \"\"\"\n",
    "        n_tags = len(self.tag2idx.keys())\n",
    "        self.bigram = np.zeros((n_tags,n_tags))\n",
    "        for tag in train_data[1]:\n",
    "            for i in range(len(tag)-1):\n",
    "                tag1 = self.tag2idx[tag[i]]\n",
    "                tag2 = self.tag2idx[tag[i+1]]\n",
    "                self.bigram[tag1,tag2]= self.bigram[tag1,tag2]+1\n",
    "        \n",
    "    \n",
    "    def get_trigrams(self):\n",
    "        \"\"\"\n",
    "        Computes trigrams. \n",
    "        Tip. Similar logic to unigrams and bigrams. Store in numpy array. \n",
    "        \"\"\"\n",
    "        n_tags = len(self.tag2idx.keys())\n",
    "        self.trigram = np.zeros((n_tags,n_tags,n_tags))\n",
    "        tags = train_data[1]\n",
    "        for tag in train_data[1]:\n",
    "            for i in range(len(tag)-2):\n",
    "                tag1 = self.tag2idx[tag[i]]\n",
    "                tag2 = self.tag2idx[tag[i+1]]\n",
    "                tag3 = self.tag2idx[tag[i+2]]\n",
    "                self.trigram[tag1,tag2,tag3]= self.trigram[tag1,tag2,tag3]+1\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_emissions(self):\n",
    "        \"\"\"\n",
    "        Computes emission probabilities. \n",
    "        Tip. Map each tag to an integer and each word in the vocabulary to an integer. \n",
    "             Then create a numpy array such that lexical[index(tag), index(word)] = Prob(word|tag) \n",
    "        \"\"\"\n",
    "        n_tags = len(self.tag2idx.keys())\n",
    "        n_words = len(self.word2idx.keys())\n",
    "        self.lexical = np.zeros((n_tags,n_words))\n",
    "        words = self.data[0]\n",
    "        tags = self.data[1]\n",
    "        n = len(words)\n",
    "        for x in range(n):\n",
    "            for i in range(len(words[x])):\n",
    "                tag_idx = self.tag2idx[tags[x][i]]\n",
    "                word_idx = self.word2idx[words[x][i]]\n",
    "                self.lexical[tag_idx,word_idx] = self.lexical[tag_idx,word_idx] + 1\n",
    "            \n",
    "    \n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Trains the model by computing transition and emission probabilities.\n",
    "\n",
    "        You should also experiment:\n",
    "            - smoothing.\n",
    "            - N-gram models with varying N.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.all_tags = list(set([t for tag in data[1] for t in tag]))\n",
    "        self.tag2idx = {self.all_tags[i]:i for i in range(len(self.all_tags))}\n",
    "        self.idx2tag = {v:k for k,v in self.tag2idx.items()}\n",
    "\n",
    "        self.all_words = list(set([w for word in data[0] for w in word]))\n",
    "        self.word2idx = {self.all_words[i]:i for i in range(len(self.all_words))}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "    \n",
    "    def compute_unigram_probability(self, alpha=1):\n",
    "        self.unigram_prob = (self.unigram + alpha) / (self.unigram.sum() + alpha * len(self.unigram))\n",
    "\n",
    "    def compute_bigram_probability(self, alpha=1):\n",
    "        self.bigram_prob = (self.bigram + alpha) / (self.bigram.sum(axis=0, keepdims=True) + alpha * self.bigram.shape[1])\n",
    "\n",
    "    def compute_trigram_probability(self, alpha=1):\n",
    "        self.trigram_prob = (self.trigram + alpha) / (self.trigram.sum(axis=(0, 1), keepdims=True) + alpha * self.trigram.shape[2])\n",
    "\n",
    "    def compute_lexical_probability(self, alpha=1):\n",
    "        self.lexical_prob = (self.lexical + alpha) / (self.lexical.sum(axis=0, keepdims=True) + alpha * self.lexical.shape[1])\n",
    "\n",
    "    def get_trigram_probabilty(self, tag1, tag2, tag3):\n",
    "        tag1_idx = self.tag2idx[tag1]\n",
    "        tag2_idx = self.tag2idx[tag2]\n",
    "        tag3_idx = self.tag2idx[tag3]\n",
    "        return self.trigram_prob[tag1_idx, tag2_idx, tag3_idx]\n",
    "\n",
    "    def get_bigram_probability(self, tag1, tag2):\n",
    "        tag1_idx = self.tag2idx[tag1]\n",
    "        tag2_idx = self.tag2idx[tag2]\n",
    "        return self.bigram_prob[tag1_idx, tag2_idx]\n",
    "\n",
    "    def get_unigram_probability(self, tag):\n",
    "        tag_idx = self.tag2idx[tag]\n",
    "        return self.unigram_prob[tag_idx]\n",
    "\n",
    "    def get_lexical_probability(self, tag, word):\n",
    "        tag_idx = self.tag2idx[tag]\n",
    "        word_idx = self.word2idx[word]\n",
    "        return self.lexical_prob[tag_idx, word_idx]\n",
    "\n",
    "    def word_probability(self, word):\n",
    "        word_idx = self.word2idx[word]\n",
    "        return (self.lexical_prob[:, word_idx] * self.unigram_prob).sum()\n",
    "\n",
    "    def sequence_probability(self, sequence, tags):\n",
    "        \"\"\"Computes the probability of a tagged sequence given the emission/transition probabilities.\"\"\"\n",
    "        probability = 1\n",
    "        probability *= self.get_unigram_probability(tags[0]) * self.get_lexical_probability(tags[0], sequence[0]) /  self.word_probability(sequence[0])\n",
    "        print(probability)\n",
    "        probability *= self.get_bigram_probability(tags[0], tags[1]) * self.get_lexical_probability(tags[1], sequence[1]) / self.word_probability(sequence[1])\n",
    "        for i in range(len(sequence) - 2):\n",
    "            probability *= self.get_trigram_probabilty(tags[i], tags[i+1], tags[i+2]) * self.get_lexical_probability(tags[i+2], sequence[i+2]) / self.word_probability(sequence[i])\n",
    "        return probability\n",
    "\n",
    "\n",
    "    def greedy(self, sequence):\n",
    "        greedy_tags = []\n",
    "        \n",
    "        tag1 = max(self.all_tags, \n",
    "                key=lambda tag: self.get_unigram_probability(tag) * self.get_lexical_probability(tag, sequence[0]))\n",
    "        greedy_tags.append(tag1)\n",
    "        \n",
    "        tag2 = max(self.all_tags, \n",
    "                key=lambda tag: self.get_bigram_probability(greedy_tags[0], tag) * self.get_lexical_probability(tag, sequence[1]))\n",
    "        greedy_tags.append(tag2)\n",
    "      \n",
    "        for i in range(2, len(sequence)):\n",
    "            tag1 = greedy_tags[i-2]\n",
    "            tag2 = greedy_tags[i-1]\n",
    "            \n",
    "            tag = max(self.all_tags, \n",
    "                    key=lambda tag: self.get_trigram_probabilty(tag1, tag2, tag) * self.get_lexical_probability(tag, sequence[i]))\n",
    "            greedy_tags.append(tag)\n",
    "        \n",
    "        return greedy_tags\n",
    "\n",
    "    def beam_k(self, sequence, k=20):\n",
    "        k_best = [\n",
    "            ([tag], self.get_unigram_probability(tag) * self.get_lexical_probability(tag, sequence[0]))\n",
    "            for tag in sorted(\n",
    "                self.all_tags,\n",
    "                key=lambda tag: self.get_unigram_probability(tag) * self.get_lexical_probability(tag, sequence[0]),\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "        ]\n",
    "\n",
    "        new_k_best = []\n",
    "        for curr_tags, curr_score in k_best:\n",
    "            new_k_top_tags = sorted(\n",
    "                [\n",
    "                    (curr_tags + [tag], curr_score * self.get_bigram_probability(curr_tags[-1], tag) * self.get_lexical_probability(tag, sequence[1]))\n",
    "                    for tag in self.all_tags\n",
    "                ],\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "            new_k_best.extend(new_k_top_tags)\n",
    "\n",
    "        k_best = sorted(new_k_best, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        for i in range(2, len(sequence)):\n",
    "            new_k_best = []\n",
    "            for curr_tags, curr_score in k_best:\n",
    "                new_k_top_tags = sorted(\n",
    "                    [\n",
    "                        (curr_tags + [tag], curr_score * self.get_trigram_probabilty(curr_tags[-2], curr_tags[-1], tag) * self.get_lexical_probability(tag, sequence[i]))\n",
    "                        for tag in self.all_tags\n",
    "                    ],\n",
    "                    key=lambda x: x[1],\n",
    "                    reverse=True\n",
    "                )[:k]\n",
    "                new_k_best.extend(new_k_top_tags)\n",
    "\n",
    "            k_best = sorted(new_k_best, key=lambda x: x[1], reverse=True)[:k]\n",
    "        best_tags = max(k_best, key=lambda x: x[1])[0]\n",
    "        return best_tags\n",
    "\n",
    "\n",
    "    def viterbi(self,sequence):\n",
    "        n_tags = len(self.tag2idx.keys())\n",
    "        n_words = len(self.word2idx.keys())\n",
    "        k = len(sequence)\n",
    "        gamma = np.zeros((k,n_tags)) #most likely tag t after k words (words=observations)\n",
    "        pre = np.zeros((k,n_tags)) #predecessor tag prev_t that maximized tag t at k\n",
    "        gamma[0,:] = [self.get_unigram_probability(tag)*self.get_lexical_probability(tag, sequence[0]) for tag in self.all_tags] #prob of tag given word\n",
    "        tag_idx = np.argmax(gamma[0])\n",
    "        print(tag_idx, self.idx2tag[tag_idx])\n",
    "        for x in range(1, k):\n",
    "            for t2 in self.all_tags:\n",
    "                t2_idx = self.tag2idx[t2]\n",
    "                gamma_probs = [\n",
    "                    (self.tag2idx[t1], gamma[x-1, self.tag2idx[t1]] * self.get_bigram_probability(t1, t2) * self.get_lexical_probability(t2, sequence[x]))\n",
    "                    for t1 in self.all_tags\n",
    "                ]\n",
    "                max_gamma_prob = max(gamma_probs, key=lambda item: item[1])\n",
    "                gamma[x, t2_idx] = max_gamma_prob[1]\n",
    "                pre[x, t2_idx] = max_gamma_prob[0]\n",
    "        \n",
    "        \n",
    "        tag_idx = np.argmax(gamma[k-1])\n",
    "        viterbi_tags = [self.idx2tag[tag_idx]]\n",
    "        for x in range(1,k):\n",
    "            tag_idx = int(pre[k-x, tag_idx])\n",
    "            viterbi_tags = [self.idx2tag[tag_idx]] + viterbi_tags\n",
    "\n",
    "        return viterbi_tags \n",
    "      \n",
    "    def inference(self, sequence):\n",
    "        \"\"\"Tags a sequence with part of speech tags.\n",
    "\n",
    "        You should implement different kinds of inference (suggested as separate\n",
    "        methods):\n",
    "\n",
    "            - greedy decoding\n",
    "            - decoding with beam search\n",
    "            - viterbi\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        greedy_tags = self.greedy(sequence)\n",
    "        print(greedy_tags)\n",
    "        beam_tags = self.beam_k(sequence)\n",
    "        print(beam_tags)\n",
    "\n",
    "        viterbi_tags = self.viterbi(sequence)\n",
    "        print(viterbi_tags)\n",
    "\n",
    "\n",
    "            \n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pos_tagger = POSTagger()\n",
    "\n",
    "    train_data = train_data\n",
    "    dev_data = dev_data\n",
    "    test_data = test_data\n",
    "\n",
    "    pos_tagger.train(train_data)\n",
    "    pos_tagger.get_unigrams()\n",
    "    pos_tagger.get_bigrams()\n",
    "    pos_tagger.get_trigrams()\n",
    "    pos_tagger.get_emissions()\n",
    "    pos_tagger.compute_unigram_probability()\n",
    "    pos_tagger.compute_bigram_probability()\n",
    "    pos_tagger.compute_trigram_probability()\n",
    "    pos_tagger.compute_lexical_probability()\n",
    "    seq = ['I', 'am','old',\"but\",\"I\",\"love\", \"you\"]\n",
    "    prob = pos_tagger.word_probability(\"I\")\n",
    "    print(prob)\n",
    "    # prob = pos_tagger.sequence_probability(seq,['PRP', 'VBP', 'JJ'])\n",
    "    \n",
    "    print(prob)\n",
    "\n",
    "    pos_tagger.inference(seq)\n",
    "    # Experiment with your decoder using greedy decoding, beam search, viterbi...\n",
    "\n",
    "\n",
    "    # Here you can also implement experiments that compare different styles of decoding,\n",
    "    # smoothing, n-grams, etc.\n",
    "    # evaluate(dev_data, pos_tagger)\n",
    "\n",
    "    # # Predict tags for the test set\n",
    "    # test_predictions = []\n",
    "    # for sentence in test_data:\n",
    "    #     test_predictions.extend(pos_tagger.inference(sentence))\n",
    "    \n",
    "    # Write them to a file to update the leaderboard\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25337660-bdab-4f9a-8630-a5573333031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-docstart-', 'Pierre', 'Vinken', ',', '61', 'years', 'old']\n",
      "['O', 'NNP', 'NNP', ',', 'CD', 'NNS', 'JJ']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0][:7])\n",
    "print(train_data[1][0][:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "142ad936-79da-4280-9bb9-eb238ec259b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((100,2,1))\n",
    "y.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "411249ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9120e+03, 9.1000e+01, 2.3940e+03, 5.1250e+03, 1.6170e+04,\n",
       "       1.4070e+03, 1.2837e+04, 6.4000e+01, 3.2170e+03, 7.2097e+04,\n",
       "       4.3879e+04, 3.5475e+04, 1.2570e+03, 2.0490e+03, 0.0000e+00,\n",
       "       2.8837e+04, 1.0300e+03, 7.2040e+03, 9.6805e+04, 6.7431e+04,\n",
       "       4.5356e+04, 6.0650e+03, 1.4622e+04, 1.5660e+03, 3.0000e+01,\n",
       "       6.4500e+02, 2.6323e+04, 3.2000e+02, 1.0210e+03, 5.9928e+04,\n",
       "       1.9353e+04, 1.2900e+02, 5.2290e+03, 2.9600e+02, 1.7410e+03,\n",
       "       6.4780e+03, 1.1013e+04, 2.2445e+04, 1.6250e+04, 5.1050e+03,\n",
       "       3.6020e+03, 2.0400e+02, 2.1357e+04, 3.7000e+01, 1.7297e+04,\n",
       "       1.3870e+03, 9.3950e+03])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.bigram.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16d365d2-e258-4f5b-9a93-3e66bb0071be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan,  1., nan, nan, nan, nan,  1.,  1.,  1., nan,\n",
       "       nan, nan,  1., nan,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,\n",
       "        1., nan, nan,  1.,  1., nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan,  1., nan])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "    def beam_k(self,sequence):\n",
    "        k = 5\n",
    "        k_best = [(tag, self.unigram[self.tag2idx[tag]] * self.lexical[self.tag2idx[tag], self.word2idx[sequence[0]]]) \n",
    "            for tag in sorted(self.all_tags, key=lambda tag: self.unigram[self.tag2idx[tag]] * self.lexical[self.tag2idx[tag], self.word2idx[sequence[0]]], reverse=True)[:k]]\n",
    "\n",
    "        new_k_best = []\n",
    "        for curr_tags, curr_score in k_best:\n",
    "            new_k_top_tags = sorted(\n",
    "                [(curr_tags, tag, curr_score * self.bigram[self.tag2idx[curr_tags[0]], self.tag2idx[tag]] * self.lexical[self.tag2idx[tag], self.word2idx[sequence[1]]]) \n",
    "                for tag in self.all_tags],\n",
    "                key=lambda x: x[2],  \n",
    "                reverse=True\n",
    "            )[:k] \n",
    "            new_k_best.extend(new_k_top_tags)\n",
    "\n",
    "        new_k_best = sorted(new_k_best, key=lambda x: x[2], reverse=True)[:k]\n",
    "        k_best = new_k_best\n",
    "\n",
    "        for i in range(2,len(sequence)):\n",
    "            new_k_best = []\n",
    "            for curr_tags, curr_score in k_best:\n",
    "                new_k_top_tags = sorted(\n",
    "                    [(curr_tags, tag, curr_score * self.trigram[self.tag2idx[curr_tags[-2]],self.tag2idx[curr_tags[-1]], self.tag2idx[tag]] * self.lexical[self.tag2idx[tag], self.word2idx[sequence[i]]]) \n",
    "                    for tag in self.all_tags],\n",
    "                    key=lambda x: x[2],  \n",
    "                    reverse=True\n",
    "                )[:k] \n",
    "                new_k_best.extend(new_k_top_tags)\n",
    "            k_best = new_k_best\n",
    "       \n",
    "        \n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
